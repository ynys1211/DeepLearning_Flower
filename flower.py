# -*- coding: utf-8 -*-
"""flower.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12wEday7krDmkSJ_PDu8TaQEJ-WqZmc4-
"""

!wget https://github.com/ynys1211/ai/raw/main/flowers.z01
!wget https://github.com/ynys1211/ai/raw/main/flowers.z02
!wget https://github.com/ynys1211/ai/raw/main/flowers.zip

!zip -F /content/flowers.zip --out /content/flowers_total.zip
!unzip flowers_total.zip

import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from keras.preprocessing.image import ImageDataGenerator

!pip install split-folders

import splitfolders

# Define the parent path and give it to splitfolders
parent_path = '/content/flowers/'

# Giving to splitfolders. Note that ratio is %80 (train), %10(test) and %10(val).
splitfolders.ratio(parent_path, output="splittedfolder", seed=213, ratio=(.8, .1, .1))

classnames = sorted(os.listdir(parent_path))
fig, axes = plt.subplots(4, 4, figsize=(15, 15))

for classname, ax in zip(classnames, axes.flatten()):
    classpath = os.path.join(parent_path, classname)
    img = os.listdir(classpath)
    rnd_img = random.choice(img)
    imgpath = os.path.join(classpath, rnd_img)

    img = tf.keras.preprocessing.image.load_img(imgpath, target_size=(128, 128))
    ax.imshow(img)
    ax.set_title(classname)
    ax.axis('off')
plt.show()

model = models.Sequential()

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())

model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(16, activation='softmax'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Summarize the network
model.summary()

trainDatagen = ImageDataGenerator(rescale= 1/255., rotation_range= 0.3, width_shift_range= 0.2, height_shift_range= 0.3,
shear_range= 0.2, zoom_range= 0.2, horizontal_flip= True, fill_mode='nearest')

testDatagen = ImageDataGenerator(rescale= 1/255.)
validationDatagen = ImageDataGenerator(rescale= 1/255.)

train = trainDatagen.flow_from_directory(directory= "/content/splittedfolder/train", batch_size= 64, class_mode= 'categorical', target_size= (128, 128), shuffle= True)
test = testDatagen.flow_from_directory(directory= "/content/splittedfolder/test", batch_size= 64, class_mode= "categorical", target_size= (128, 128))
validation = validationDatagen.flow_from_directory(directory= "/content/splittedfolder/val", batch_size= 64, class_mode= "categorical", target_size= (128, 128))

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())

model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dropout(0.5))  # Adding dropout layer
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.5))  # Adding dropout layer
model.add(layers.Dense(16, activation='softmax'))

# Compile the model
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

optimizer = learning_rate=0.0001


# Summarize the network
model.summary()

from tensorflow.keras import models, layers, optimizers

def residual_block(x, filters, kernel_size=3, strides=1):
    """Residual block with two convolutional layers"""
    y = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(x)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)

    y = layers.Conv2D(filters, kernel_size, padding='same')(y)
    y = layers.BatchNormalization()(y)

    # Skip connection
    if strides > 1:
        x = layers.Conv2D(filters, kernel_size=1, strides=strides, padding='same')(x)
    out = layers.Add()([x, y])
    out = layers.Activation('relu')(out)
    return out

def build_resnet(input_shape, num_classes):
    """Build ResNet model"""
    inputs = layers.Input(shape=input_shape)

    # Initial convolutional layer
    x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # Residual blocks
    x = residual_block(x, 32, strides=1)
    x = residual_block(x, 32, strides=1)
    x = residual_block(x, 64, strides=2)  # Downsample
    x = residual_block(x, 64, strides=1)
    x = residual_block(x, 128, strides=2)  # Downsample
    x = residual_block(x, 128, strides=1)

    # Global average pooling and output
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    # Create model
    model = models.Model(inputs, outputs)
    return model

# Define input shape and number of classes
input_shape = (128, 128, 3)
num_classes = 16

# Build ResNet model
model = build_resnet(input_shape, num_classes)

# Compile model
optimizer = optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

def scheduler(epoch, lr) :
  if epoch < 5 :
    return lr
  else :
    return lr * tf.math.exp(-0.1)

lr_scheduler = LearningRateScheduler(scheduler)

checkpoint_acc = ModelCheckpoint('best_model_acc.h5', save_best_only=True, monitor='val_accuracy', mode='max')
checkpoint_loss = ModelCheckpoint('best_model_loss.h5', save_best_only=True, monitor='val_loss', mode='min')

history = model.fit(train, epochs=140, batch_size=64, validation_data=validation, verbose='auto', callbacks=[checkpoint_acc,checkpoint_loss,lr_scheduler])

model.evaluate(test)

train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Create a subplot with 1 row and 2 columns
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot the accuracy graph on the first subplot
ax1.plot(train_acc, label='Training Accuracy')
ax1.plot(val_acc, label='Validation Accuracy')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Accuracy')
ax1.legend(loc='best')
ax1.set_title('Training and Validation Accuracy')

# Plot the loss graph on the second subplot
ax2.plot(train_loss, label='Training Loss')
ax2.plot(val_loss, label='Validation Loss')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Loss')
ax2.legend(loc='best')
ax2.set_title('Training and Validation Loss')

# Display the graphs
plt.show()